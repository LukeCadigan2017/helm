{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/crfm-helm2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "PyTorch version 2.6.0 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: process_gens_mode ['wmt_single', 'wmt_top_k']\n",
      "calculate_gen_summary_dict\n",
      "snellius_copies/helm_output/sample_10_eval_1000/wmt_14_language_pair_de_en_/meta_llama_Llama_3.1_8B_Instruct/1_beams/runs/sample_10_eval_1000\n",
      "get_metrics_dict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/crfm-helm2/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/miniconda3/envs/crfm-helm2/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/miniconda3/envs/crfm-helm2/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate_gen_summary_dict\n",
      "snellius_copies/helm_output/sample_10_eval_1000_top_k_30/wmt_14_language_pair_de_en_/meta_llama_Llama_3.1_8B_Instruct/1_beams/runs/sample_10_eval_1000_top_k_30\n",
      "get_metrics_dict\n",
      "Index(['example_comet', 'beam_num', 'task_name', 'model', 'example_idx',\n",
      "       'suite', 'instanceID', 'text', 'completion_length', 'output_logprob',\n",
      "       'isCompletion', 'BLEU_1', 'BLEU_4'],\n",
      "      dtype='object')\n",
      "Num examples: 20000\n",
      "Num completions: 2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "from process_gens import *\n",
    "\n",
    "import pandas as pd\n",
    "from helm.benchmark.runner import InstanceGenerations,GenerationSummary\n",
    "from typing import Any, List\n",
    "import json\n",
    "from helm.common.request import (GeneratedOutput, Token)\n",
    "\n",
    "import PostMetric\n",
    "import pandas as pd\n",
    "\n",
    "from helm.benchmark.metrics.statistic import Stat\n",
    "from typing import Dict, Optional\n",
    "\n",
    "from helm.benchmark.augmentations.perturbation_description import (\n",
    "    PerturbationDescription)\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from process_gen_utils import *\n",
    "\n",
    "\n",
    "\n",
    "processGens=ProcessGens()\n",
    "\n",
    "#gsm modes\n",
    "# process_gen_mode=\"llama_gsm_sample\"\n",
    "# compare_metric='final_num_exact_match'\n",
    "\n",
    "\n",
    "#wmt modes\n",
    "process_gen_modes=[\"wmt_single\", \"wmt_top_k\"]\n",
    "compare_metric=\"example_comet\"\n",
    "# process_gen_mode=\"wmt_beam128\"\n",
    "# compare_metric=\"BLEU_4\"\n",
    "\n",
    "#instruct modes\n",
    "# process_gen_mode=\"instruct8\"\n",
    "# compare_metric=\"example_themis\"\n",
    "\n",
    "\n",
    "do_norm_analysis=True\n",
    "\n",
    "processGens.init_with_mode(process_gen_modes, print_files=True)\n",
    "\n",
    "\n",
    "root_folder, num_beams_list, models, custom_metrics, task_names, suite_name, instance_metrics = processGens.get_params()\n",
    "\n",
    "\n",
    "examples_df, completions_df=get_dfs(processGens)\n",
    "\n",
    "print(len(completions_df))\n",
    "\n",
    "\n",
    "if(do_norm_analysis):\n",
    "    for col in [compare_metric, 'output_logprob']:\n",
    "        examples_df[col + '_norm'] = examples_df.groupby('instanceID')[col].transform(\n",
    "            lambda x: (x - x.mean()) / x.std()\n",
    "        )\n",
    "\n",
    "dfs_by_model={}\n",
    "for model_idx, model_name in enumerate(models):\n",
    "    filtered_df = examples_df[examples_df[\"model\"]==model_idx]\n",
    "    dfs_by_model[model_name]=filtered_df\n",
    "\n",
    "\n",
    "all_dfs_by_model=dfs_by_model\n",
    "if(len(models)>1):\n",
    "    all_dfs_by_model[\"all_models\"]=examples_df\n",
    "\n",
    "\n",
    "\n",
    "# code snellius_copies/helm_output/sample_10_eval_1000/wmt_14_language_pair_de_en_/meta_llama_Llama_3.1_8B_Instruct/1_beams/runs/sample_10_eval_1000/generation_summary_metrics.json\n",
    "# code snellius_copies/helm_output/sample_10_eval_20_top_k_2/wmt_14_language_pair_de_en_/meta_llama_Llama_3.1_8B_Instruct/1_beams/runs/sample_10_eval_20_top_k_2/generation_summary_metrics.json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t root_folder: snellius_copies/helm_output \n",
      " \t num_beams_list [1] \n",
      " \t models: ['meta_llama_Llama_3.1_8B_Instruct'] \n",
      " \t custom_metrics['BLEU_1', 'BLEU_4']\n",
      " \t task_names ['wmt_14_language_pair_de_en_'] \n",
      " \t suite_name sample_10_eval_1000_top_k_30 \n",
      " \t instance_metrics ['comet']\n",
      "   example_comet  beam_num                    task_name  \\\n",
      "0        0.81154         1  wmt_14_language_pair_de_en_   \n",
      "1        0.81795         1  wmt_14_language_pair_de_en_   \n",
      "\n",
      "                              model  example_idx                suite  \\\n",
      "0  meta_llama_Llama_3.1_8B_Instruct            0  sample_10_eval_1000   \n",
      "1  meta_llama_Llama_3.1_8B_Instruct            1  sample_10_eval_1000   \n",
      "\n",
      "   instanceID                                               text  \\\n",
      "0           0  He advised the parents of a boy whose penis ha...   \n",
      "1           0  He advised the parents of a boy whose penis ha...   \n",
      "\n",
      "   completion_length  output_logprob isCompletion    BLEU_1    BLEU_4  \\\n",
      "0                196      -16.225747         True  0.729544  0.422440   \n",
      "1                183      -27.334976         None  0.657895  0.314286   \n",
      "\n",
      "   example_comet_norm  output_logprob_norm  \n",
      "0           -0.841489             0.868141  \n",
      "1           -0.642365            -0.477183  \n",
      "num examples 20000\n",
      "num_completions 2000\n",
      "compare_metric mean:  example_comet    0.769441\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\t root_folder: {root_folder} \\n \\t num_beams_list {num_beams_list} \\n \\t models: {models} \\n \\t custom_metrics{[metric.name() for metric in custom_metrics]}\"+ \n",
    "    f\"\\n \\t task_names {task_names} \\n \\t suite_name {suite_name} \\n \\t instance_metrics {instance_metrics}\")\n",
    "instanceGeneration=get_first(processGens.first_run_instances)\n",
    "print(examples_df.head(2))\n",
    "print(\"num examples\" ,len(examples_df))\n",
    "print(\"num_completions\", len(completions_df))\n",
    "print(f\"compare_metric mean: \",examples_df[[compare_metric]].mean())\n",
    "#current belief: for each run_folder combo, there will be 100 instances and 100 examples per instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'sample_10_eval_20_top_k_2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/envs/crfm-helm2/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sample_10_eval_20_top_k_2'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(examples_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstanceID\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique())\n\u001b[1;32m      5\u001b[0m raw_average_probs\u001b[38;5;241m=\u001b[39mexamples_df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstanceID\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuite\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_logprob\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuite\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstanceID\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mreset_index()\u001b[38;5;241m.\u001b[39mpivot(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstanceID\u001b[39m\u001b[38;5;124m\"\u001b[39m,columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuite\u001b[39m\u001b[38;5;124m\"\u001b[39m, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_logprob\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m average_probs\u001b[38;5;241m=\u001b[39m raw_average_probs[\u001b[43mraw_average_probs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msample_10_eval_20_top_k_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mnotnull()]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(average_probs\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m21\u001b[39m))\n",
      "File \u001b[0;32m/opt/miniconda3/envs/crfm-helm2/lib/python3.9/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/crfm-helm2/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sample_10_eval_20_top_k_2'"
     ]
    }
   ],
   "source": [
    "# pivoted = completions_df.pivot(index='instanceID', columns='beam_num', values=compare_metric)\n",
    "\n",
    "\n",
    "print(examples_df[\"instanceID\"].nunique())\n",
    "raw_average_probs=examples_df[[\"instanceID\",\"suite\", \"output_logprob\"]].groupby([\"suite\", \"instanceID\"]).mean().reset_index().pivot(index=\"instanceID\",columns=\"suite\", values=\"output_logprob\")\n",
    "\n",
    "average_probs= raw_average_probs[raw_average_probs[\"sample_10_eval_20_top_k_2\"].notnull()]\n",
    "\n",
    "# .pivot(index=\"instanceID\", columns=\"suite\", values=\"output_logprob\")\n",
    "\n",
    "# groupby([\"suite\"]).mean()\n",
    "# print(average_probs.head(2))\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plots_wide=1\n",
    "# num_models=len(dfs_by_model.keys())\n",
    "# plots_tall=math.ceil(num_models/plots_wide)\n",
    "\n",
    "# suptitle=\"\"\n",
    "# axs=None\n",
    "# fig=None\n",
    "\n",
    "# for idx, (model_name, filtered_df) in enumerate(dfs_by_model.items()):\n",
    "    \n",
    "#     if axs is None:\n",
    "#         ax=None\n",
    "#     else:\n",
    "#         title=model_name\n",
    "#         if(num_models>1):\n",
    "#             axs_x=idx % plots_wide\n",
    "#             axs_y= math.floor(idx / plots_wide)\n",
    "#             ax=axs[axs_x, axs_y]\n",
    "#         else:\n",
    "#             ax=axs\n",
    "\n",
    "    \n",
    "#     def calculate_title():\n",
    "#         return f\"{model_name}: {suptitle}\" if ax is None else model_name\n",
    "#     # plot by rank within sentence\n",
    "#     suptitle=\"grouped by rank within sentence\"\n",
    "#     plot_grouped(df=filtered_df, xlabel=\"output_logprob\", groupby='example_idx', ylabel=compare_metric, title=calculate_title(), ax=ax)\n",
    "    \n",
    "\n",
    "#     # plot \n",
    "    \n",
    "#     # suptitle=\"Raw Plotting Probability vs Quality\"\n",
    "#     # plot_keys(df=examples_df, xlabel='output_logprob', ylabel=compare_metric, title=calculate_title(), ax=ax)\n",
    "\n",
    "\n",
    "#     # # just plot metric / probability (normalized) \n",
    "#     # suptitle=\"Normalized Probability vs Quality\"\n",
    "#     # plot_keys(df=filtered_df, xlabel='output_logprob_norm', ylabel=compare_metric+'_norm', title=calculate_title(), ax=ax)\n",
    "\n",
    "#     # # plot: group into equally-sized bins (ignores examples example_id)\n",
    "#     # suptitle=\"Grouped with equal bins (by logprob)\"\n",
    "#     # plot_grouped(df=filtered_df, xlabel=\"output_logprob\", groupby=\"bins\", ylabel=compare_metric, title=calculate_title(), ax=ax, nbins=10)\n",
    "\n",
    "#     # suptitle=\"Grouped with equal bins (by logprob)\"\n",
    "#     # plot_grouped(df=filtered_df, xlabel='output_logprob_norm',  groupby=\"bins\", ylabel=compare_metric+'_norm', title=calculate_title(), ax=ax, nbins=10)\n",
    "\n",
    "#     # # suptitle=\"Grouped by reference sentence\"\n",
    "#     # instance_df=filtered_df.groupby(\"instanceID\")[[\"output_logprob\", compare_metric]].mean()\n",
    "#     # suptitle=\"Grouped by reference sentence\"\n",
    "#     # plot_keys(df=instance_df, xlabel='output_logprob', ylabel=compare_metric, title=calculate_title(), ax=ax)\n",
    "    \n",
    "#     # suptitle=\"Grouped by reference sentence\"\n",
    "#     # plot_grouped(df=instance_df, xlabel='output_logprob',  groupby=\"bins\", ylabel=compare_metric, title=calculate_title(), ax=ax, nbins=10)\n",
    "    \n",
    "# if fig is not None:\n",
    "#     fig.suptitle(title)\n",
    "#     fig.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crfm-helm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
