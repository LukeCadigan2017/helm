{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# checkpoint = \"google-t5/t5-small\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_name=\"distilbert/distilgpt2\"\n",
    "# model_name= \"google-t5/t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "encoded_input = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(device)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_generated=6\n",
    "batch_size=2\n",
    "length_penalty=0\n",
    "top_p=1\n",
    "top_k=0\n",
    "temperature=1\n",
    "raw_request={\n",
    "    \"max_new_tokens\":10\n",
    "}\n",
    "optional_args={}\n",
    "stopping_criteria=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 19])\n",
      "10\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/crfm-helm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:698: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `0` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch!\n",
      "torch.Size([2, 19])\n",
      "torch.Size([10, 2, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch!\n",
      "torch.Size([2, 19])\n",
      "torch.Size([10, 2, 50257])\n",
      "batch!\n",
      "torch.Size([2, 19])\n",
      "torch.Size([10, 2, 50257])\n",
      "Final!\n",
      "torch.Size([6, 19])\n",
      "torch.Size([10, 6, 50257])\n"
     ]
    }
   ],
   "source": [
    "batch_size = num_generated if batch_size is None else batch_size\n",
    "assert (num_generated % batch_size)==0\n",
    "logits=None\n",
    "sequences=None\n",
    "for i in range(int(num_generated / batch_size)):\n",
    "    batch_output = model.generate(\n",
    "        **encoded_input,\n",
    "        num_return_sequences=batch_size,\n",
    "        max_new_tokens=raw_request[\"max_new_tokens\"],\n",
    "\n",
    "        length_penalty=length_penalty,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        do_sample=True,\n",
    "\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        output_logits=True,\n",
    "        **optional_args,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "    )\n",
    "\n",
    "\n",
    "    #generate\n",
    "    batch_sequences = batch_output.sequences\n",
    "    batch_logits = torch.stack(list(batch_output.logits), dim=0)\n",
    "\n",
    "    print(\"batch!\")\n",
    "    print(batch_sequences.size())\n",
    "    print(batch_logits.size())\n",
    "\n",
    "    def safe_append_tensor(tensor_agg, batch_tensor, axis):\n",
    "        if tensor_agg is None:\n",
    "            return batch_tensor\n",
    "        return  torch.cat((tensor_agg,batch_tensor), axis=axis)\n",
    "\n",
    "    sequences = safe_append_tensor(sequences, batch_sequences, 0)\n",
    "    logits = safe_append_tensor(logits, batch_logits, 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tensor_batch_logits = np.asarray(batch_logits)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crfm-helm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
