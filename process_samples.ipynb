{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: process_gens_mode wmt_samples\n",
      "calculate_gen_summary_dict\n",
      "get_metrics_dict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/crfm-helm2/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/miniconda3/envs/crfm-helm2/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/miniconda3/envs/crfm-helm2/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['example_comet', 'beam_num', 'task_name', 'model', 'example_idx',\n",
      "       'text', 'completion_length', 'output_logprob', 'instanceID',\n",
      "       'isCompletion', 'BLEU_1', 'BLEU_4'],\n",
      "      dtype='object')\n",
      "Num examples: 50000\n",
      "Num completions: 5000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import process_gens\n",
    "import pandas as pd\n",
    "from helm.benchmark.runner import InstanceGenerations,GenerationSummary\n",
    "from typing import Any, List\n",
    "import json\n",
    "from helm.common.request import (GeneratedOutput, Token)\n",
    "\n",
    "import PostMetric\n",
    "import pandas as pd\n",
    "\n",
    "from helm.benchmark.metrics.statistic import Stat\n",
    "from typing import Dict, Optional\n",
    "\n",
    "from helm.benchmark.augmentations.perturbation_description import (\n",
    "    PerturbationDescription)\n",
    "from dataclasses import dataclass\n",
    "from process_gens import *\n",
    "from process_gen_utils import *\n",
    "\n",
    "\n",
    "\n",
    "processGens=ProcessGens()\n",
    "\n",
    "#gsm modes\n",
    "# process_gen_mode=\"llama_gsm_sample\"\n",
    "# compare_metric='final_num_exact_match'\n",
    "\n",
    "\n",
    "#wmt modes\n",
    "process_gen_mode=\"wmt_samples\"\n",
    "# process_gen_mode=\"wmt_beam8\"\n",
    "compare_metric=\"example_comet\"\n",
    "\n",
    "# process_gen_mode=\"wmt_beam128\"\n",
    "# compare_metric=\"BLEU_4\"\n",
    "\n",
    "#instruct modes\n",
    "# process_gen_mode=\"instruct8\"\n",
    "# compare_metric=\"example_themis\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "do_norm_analysis=True\n",
    "processGens.init_with_mode(process_gen_mode)\n",
    "\n",
    "\n",
    "root_folder, num_beams_list, models, custom_metrics, task_names, suite_name, instance_metrics = processGens.get_params()\n",
    "\n",
    "examples_df, completions_df=get_dfs(processGens, num_beams_list)\n",
    "\n",
    "\n",
    "if(do_norm_analysis):\n",
    "    for col in [compare_metric, 'output_logprob']:\n",
    "        examples_df[col + '_norm'] = examples_df.groupby('instanceID')[col].transform(\n",
    "            lambda x: (x - x.mean()) / x.std()\n",
    "        )\n",
    "\n",
    "dfs_by_model={}\n",
    "for model_idx, model_name in enumerate(models):\n",
    "    filtered_df = examples_df[examples_df[\"model\"]==model_idx]\n",
    "    dfs_by_model[model_name]=filtered_df\n",
    "\n",
    "\n",
    "all_dfs_by_model=dfs_by_model\n",
    "if(len(models)>1):\n",
    "    all_dfs_by_model[\"all_models\"]=examples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t root_folder: snellius_copies/helm_output \n",
      " \t num_beams_list [1] \n",
      " \t models: ['allenai_OLMo_2_0425_1B_Instruct', 'allenai_OLMo_2_1124_7B_Instruct', 'allenai_OLMo_2_1124_13B_Instruct', 'meta_llama_Llama_3.2_1B_Instruct', 'meta_llama_Llama_3.1_8B_Instruct'] \n",
      " \t custom_metrics['BLEU_1', 'BLEU_4']\n",
      " \t task_names ['wmt_14_language_pair_de_en_'] \n",
      " \t suite_name sample_10_eval_1000 \n",
      " \t instance_metrics ['comet']\n",
      "   example_comet  beam_num  task_name  model  example_idx  \\\n",
      "0       0.300900         1          0      0            0   \n",
      "1       0.305558         1          0      0            1   \n",
      "\n",
      "                                                text  completion_length  \\\n",
      "0  Input: a list of numbers 2 to 10 inclusive\\n\\n...                123   \n",
      "1  Σημαντικότητα: είναι βασικός γεωκρατικός δημιο...                155   \n",
      "\n",
      "   output_logprob instanceID isCompletion    BLEU_1         BLEU_4  \\\n",
      "0      -67.308530    id24245         True  0.170227  2.056165e-308   \n",
      "1     -142.539153    id24245         None  0.052632  2.225074e-308   \n",
      "\n",
      "   example_comet_norm  output_logprob_norm  \n",
      "0           -1.687838            -0.083452  \n",
      "1           -1.667677            -1.027302  \n",
      "num examples 50000\n",
      "num_completions 5000\n",
      "compare_metric mean:  example_comet    0.662791\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\t root_folder: {root_folder} \\n \\t num_beams_list {num_beams_list} \\n \\t models: {models} \\n \\t custom_metrics{[metric.name() for metric in custom_metrics]}\"+ \n",
    "    f\"\\n \\t task_names {task_names} \\n \\t suite_name {suite_name} \\n \\t instance_metrics {instance_metrics}\")\n",
    "instanceGeneration=get_first(processGens.first_run_instances)\n",
    "print(examples_df.head(2))\n",
    "print(\"num examples\" ,len(examples_df))\n",
    "print(\"num_completions\", len(completions_df))\n",
    "print(f\"compare_metric mean: \",examples_df[[compare_metric]].mean())\n",
    "#current belief: for each run_folder combo, there will be 100 instances and 100 examples per instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfs_by_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m plots_wide\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 2\u001b[0m num_models\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[43mdfs_by_model\u001b[49m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      3\u001b[0m plots_tall\u001b[38;5;241m=\u001b[39mmath\u001b[38;5;241m.\u001b[39mceil(num_models\u001b[38;5;241m/\u001b[39mplots_wide)\n\u001b[1;32m      5\u001b[0m suptitle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfs_by_model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "plots_wide=1\n",
    "num_models=len(dfs_by_model.keys())\n",
    "plots_tall=math.ceil(num_models/plots_wide)\n",
    "\n",
    "suptitle=\"\"\n",
    "axs=None\n",
    "fig=None\n",
    "\n",
    "for idx, (model_name, filtered_df) in enumerate(dfs_by_model.items()):\n",
    "    \n",
    "    if axs is None:\n",
    "        ax=None\n",
    "    else:\n",
    "        title=model_name\n",
    "        if(num_models>1):\n",
    "            axs_x=idx % plots_wide\n",
    "            axs_y= math.floor(idx / plots_wide)\n",
    "            ax=axs[axs_x, axs_y]\n",
    "        else:\n",
    "            ax=axs\n",
    "\n",
    "    \n",
    "    def calculate_title():\n",
    "        return f\"{model_name}: {suptitle}\" if ax is None else model_name\n",
    "    # plot by rank within sentence\n",
    "    suptitle=\"grouped by rank within sentence\"\n",
    "    plot_grouped(df=filtered_df, xlabel=\"output_logprob\", groupby='example_idx', ylabel=compare_metric, title=calculate_title(), ax=ax)\n",
    "    \n",
    "\n",
    "    # plot \n",
    "    \n",
    "    # suptitle=\"Raw Plotting Probability vs Quality\"\n",
    "    # plot_keys(df=examples_df, xlabel='output_logprob', ylabel=compare_metric, title=calculate_title(), ax=ax)\n",
    "\n",
    "\n",
    "    # # just plot metric / probability (normalized) \n",
    "    # suptitle=\"Normalized Probability vs Quality\"\n",
    "    # plot_keys(df=filtered_df, xlabel='output_logprob_norm', ylabel=compare_metric+'_norm', title=calculate_title(), ax=ax)\n",
    "\n",
    "    # # plot: group into equally-sized bins (ignores examples example_id)\n",
    "    # suptitle=\"Grouped with equal bins (by logprob)\"\n",
    "    # plot_grouped(df=filtered_df, xlabel=\"output_logprob\", groupby=\"bins\", ylabel=compare_metric, title=calculate_title(), ax=ax, nbins=10)\n",
    "\n",
    "    # suptitle=\"Grouped with equal bins (by logprob)\"\n",
    "    # plot_grouped(df=filtered_df, xlabel='output_logprob_norm',  groupby=\"bins\", ylabel=compare_metric+'_norm', title=calculate_title(), ax=ax, nbins=10)\n",
    "\n",
    "    # # suptitle=\"Grouped by reference sentence\"\n",
    "    # instance_df=filtered_df.groupby(\"instanceID\")[[\"output_logprob\", compare_metric]].mean()\n",
    "    # suptitle=\"Grouped by reference sentence\"\n",
    "    # plot_keys(df=instance_df, xlabel='output_logprob', ylabel=compare_metric, title=calculate_title(), ax=ax)\n",
    "    \n",
    "    # suptitle=\"Grouped by reference sentence\"\n",
    "    # plot_grouped(df=instance_df, xlabel='output_logprob',  groupby=\"bins\", ylabel=compare_metric, title=calculate_title(), ax=ax, nbins=10)\n",
    "    \n",
    "if fig is not None:\n",
    "    fig.suptitle(title)\n",
    "    fig.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crfm-helm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
