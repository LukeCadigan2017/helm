{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/crfm-helm2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "PyTorch version 2.6.0 available.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from helm.benchmark.runner import InstanceGenerations,GenerationSummary\n",
    "from typing import Any, List\n",
    "import json\n",
    "from helm.common.request import (GeneratedOutput, Token)\n",
    "\n",
    "import PostMetric\n",
    "import pandas as pd\n",
    "\n",
    "from helm.benchmark.metrics.statistic import Stat\n",
    "from typing import Dict, Optional\n",
    "\n",
    "from helm.benchmark.augmentations.perturbation_description import (\n",
    "    PerturbationDescription)\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def print_keys(element, depth=0):\n",
    "\n",
    "    # print(f\"element: {type(element)}\")\n",
    "    \n",
    "    if isinstance(element, dict) and element:\n",
    "        first_key=next(iter(element.keys()))\n",
    "        print(f\"key {depth} example: {first_key}\")\n",
    "        print_keys(element[first_key], depth+1)\n",
    "    else:\n",
    "        print(f\"value is {element}\")\n",
    "\n",
    "\n",
    "def fix_example_themis(completionExample):\n",
    "    \n",
    "    if(completionExample.stats_dict and \"example_themis\" in completionExample.stats_dict.keys()):\n",
    "        def parse(evaluation):\n",
    "         \n",
    "            match = re.search(r'\\brating\\s*:\\s*([1-5])\\b', evaluation, re.IGNORECASE)\n",
    "\n",
    "            if match:\n",
    "                rating = int(match.group(1))\n",
    "                return rating\n",
    "            else:\n",
    "                return -1\n",
    "        rating = parse(completionExample.evaluation)\n",
    "        rating = rating if rating is not None else -1\n",
    "        completionExample.stats_dict[\"example_themis\"]=rating\n",
    "    return completionExample\n",
    "\n",
    "\n",
    "\n",
    "def get_model_details(model_name):\n",
    "\n",
    "\n",
    "    info_dict={\n",
    "        \"allenai_OLMo_2_0425_1B_Instruct\":{\"size\": 1, \"suite\":  \"olmo\",\"model_type\":\"instruct\"},\n",
    "        \"allenai_OLMo_2_1124_7B_Instruct\":{\"size\": 7, \"suite\":  \"olmo\",\"model_type\":\"instruct\"},\n",
    "        \"allenai_OLMo_2_1124_13B_Instruct\":{\"size\": 13, \"suite\":  \"olmo\",\"model_type\":\"instruct\"},\n",
    "        \"meta_llama_Llama_3.2_1B_Instruct\":{\"size\": 1, \"suite\": \"llama\",\"model_type\":\"instruct\"},\n",
    "        \"meta_llama_Llama_3.1_8B_Instruct\":{\"size\": 8, \"suite\": \"llama\",\"model_type\":\"instruct\"},\n",
    "    }\n",
    "    \n",
    "    return info_dict[model_name]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_process_gen_params(test_name):\n",
    "\n",
    "    def get_metrics(mode):\n",
    "        if(mode==\"wmt\"):\n",
    "            task_names=[\"wmt_14_language_pair_de_en_\"]\n",
    "            custom_metrics=[ PostMetric.BLEU1_METRIC(),PostMetric.BLEU4_METRIC()]\n",
    "            instance_metrics=[\"comet\"]\n",
    "        elif(mode==\"gsm\"):\n",
    "            task_names=[\"gsm_\"]\n",
    "            custom_metrics=[PostMetric.EXAMPLE_FINAL_NUM_EXACT_MATCH_METRIC()]\n",
    "            instance_metrics=[]\n",
    "            # instance_metrics=[\"exact_match_indicator\",\"final_number_exact_match\"]\n",
    "        elif(mode==\"instruct\"):\n",
    "            print(\"\\n\\n----------------\\n NOTE: ONLY PRINTING 4 tasks ----------------\\n\")\n",
    "            # task_names=[\"open_assistant:language=en,num_respondents=1,\",\"self_instruct:num_respondents=1,\"]\n",
    "            task_names=[\n",
    "                        \"anthropic_hh_rlhf_subset_hh_num_respondents_1_\",\n",
    "                         \"koala_num_respondents_1_\", \n",
    "                        \"anthropic_hh_rlhf_subset_red_team_num_respondents_1_\",\n",
    "                        \"self_instruct_num_respondents_1_\",\n",
    "                        \"grammar_path_src_helm_benchmark_scenarios_best_chatgpt_prompts.yaml_tags_num_respondents_1_\",\n",
    "                        \"vicuna_num_respondents_1_\"]\n",
    "            custom_metrics=[]\n",
    "            instance_metrics=[]\n",
    "        else:\n",
    "            raise Exception(f\"Did not recognize mode {mode}\")\n",
    "        assert isinstance(task_names, list)\n",
    "        assert isinstance(task_names[0],str)\n",
    "        return task_names, custom_metrics, instance_metrics\n",
    "\n",
    "    root_folder=f\"snellius_copies/helm_output\"\n",
    "    if(test_name==\"wmt_samples\"):\n",
    "        mode = \"wmt\"\n",
    "        # suite_name=\"sample_return_20_eval_500\"\n",
    "        # suite_name=\"sample_return_100_eval_100\"\n",
    "        suite_name=\"sample_10_eval_1000\"\n",
    "        num_beams_list=[1]\n",
    "        # models=[\"meta_llama_Llama_3.1_8B_Instruct\"]\n",
    "        models=[\"allenai_OLMo_2_0425_1B_Instruct\",\"allenai_OLMo_2_1124_7B_Instruct\",\"allenai_OLMo_2_1124_13B_Instruct\",\"meta_llama_Llama_3.2_1B_Instruct\",\"meta_llama_Llama_3.1_8B_Instruct\"]\n",
    "\n",
    "    elif(test_name==\"wmt_single\"):\n",
    "        mode = \"wmt\"\n",
    "        # suite_name=\"sample_return_20_eval_500\"\n",
    "        # suite_name=\"sample_return_100_eval_100\"\n",
    "        suite_name=\"sample_10_eval_1000\"\n",
    "        num_beams_list=[1]\n",
    "        models=[\"meta_llama_Llama_3.1_8B_Instruct\"]\n",
    "\n",
    "    elif(test_name==\"wmt_single_top_k_2\"):\n",
    "        mode = \"wmt\"\n",
    "        suite_name=\"sample_10_eval_20_top_k_2\"\n",
    "        num_beams_list=[1]\n",
    "        models=[\"meta_llama_Llama_3.1_8B_Instruct\"]\n",
    "\n",
    "    elif(test_name==\"wmt_sample_50\"):\n",
    "\n",
    "        root_folder=\"snellius_copies/helm_output/notable_samples\"\n",
    "        mode = \"wmt\"\n",
    "        # suite_name=\"sample_return_20_eval_500\"\n",
    "        suite_name=\"sample_return_100_eval_100\"\n",
    "        num_beams_list=[1]\n",
    "        models=[\"meta_llama_Llama_3.1_8B_Instruct\"]\n",
    "\n",
    "    elif(test_name==\"wmt_sample_100\"):\n",
    "\n",
    "        root_folder=\"snellius_copies/helm_output/notable_samples\"\n",
    "        mode = \"wmt\"\n",
    "        # suite_name=\"sample_return_20_eval_500\"\n",
    "        suite_name=\"sample_return_100_eval_100\"\n",
    "        num_beams_list=[1]\n",
    "        models=[\"meta_llama_Llama_3.1_8B_Instruct\"]\n",
    "\n",
    "    elif(test_name==\"wmt_beam8\"):\n",
    "        mode = \"wmt\"\n",
    "        suite_name=\"sample_1_eval_1000\"\n",
    "        num_beams_list=[8]\n",
    "        models=[\"meta_llama_Llama_3.1_8B_Instruct\", \"allenai_OLMo_2_1124_13B_Instruct\"]\n",
    "\n",
    "    \n",
    "    elif(test_name==\"wmt_beam128\"):\n",
    "        mode = \"wmt\"\n",
    "        suite_name=\"sample_1_eval_1000\"\n",
    "        num_beams_list=[128]\n",
    "        models=[\"meta_llama_Llama_3.1_8B_Instruct\"]\n",
    "\n",
    "    # elif(test_name==\"wmt_beam8_new\"):\n",
    "    #     mode = \"wmt\"\n",
    "    #     suite_name=\"full_wmt_1_samples_1000_evals\"\n",
    "    #     num_beams_list=[16]\n",
    "    #     models=[\"meta_llama_Llama_3.1_8B_Instruct\"]\n",
    "\n",
    "    elif (test_name==\"full_instruct\"):\n",
    "        mode=\"instruct\"\n",
    "        suite_name=\"full_instruct_1_samples_100_evals\"\n",
    "        num_beams_list=[2,4,8]\n",
    "        models=[\"allenai_OLMo_2_1124_13B_Instruct\"]\n",
    "\n",
    "    elif (test_name==\"instruct8\"):\n",
    "        mode=\"instruct\"\n",
    "        suite_name=\"full_instruct_1_samples_100_evals\"\n",
    "        num_beams_list=[8]\n",
    "        models=[\"allenai_OLMo_2_1124_13B_Instruct\"]\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    ###### INDIVIDUAL TESTS  ######\n",
    "    elif(test_name==\"llama_gsm_sample\"):\n",
    "        mode = \"gsm\"\n",
    "        suite_name=\"sample_10_eval_1000\"\n",
    "        num_beams_list=[1]\n",
    "        models=[\"meta_llama_Llama_3.1_8B_Instruct\"]\n",
    "        \n",
    "    elif(test_name==\"olmo_wmt\"):\n",
    "        mode = \"wmt\"\n",
    "        suite_name=\"full_wmt_1_samples_1000_evals\"\n",
    "        num_beams_list=[2,4,8,16]\n",
    "        models=[\"allenai_OLMo_2_1124_13B_Instruct\"]\n",
    "        \n",
    "    \n",
    "    elif(test_name==\"olmo_gsm\"):\n",
    "        mode = \"gsm\"\n",
    "        suite_name=\"full_wmt_1_samples_1000_evals\"\n",
    "        num_beams_list=[2,4,8]\n",
    "        models=[\"allenai_OLMo_2_1124_13B_Instruct\"]\n",
    "\n",
    "    else:\n",
    "        except_str=f\"task name {test_name} not found\"\n",
    "        print(except_str)\n",
    "        raise Exception(except_str)\n",
    "    \n",
    "    task_names, custom_metrics, instance_metrics= get_metrics(mode)\n",
    "    return root_folder, num_beams_list, models, custom_metrics, task_names, suite_name, instance_metrics\n",
    "    \n",
    "\n",
    "@dataclass(frozen=False)\n",
    "class PerInstanceStats:\n",
    "    \"\"\"\n",
    "    Captures a unit of evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Uniquely identifies the input instance\n",
    "    instance_id: str\n",
    "    train_trial_index: int\n",
    "    \"\"\"Which replication\"\"\"\n",
    "\n",
    "    stats: List[Stat]\n",
    "    \"\"\"Statistics computed from the predicted output\"\"\"\n",
    "    perturbation: Optional[PerturbationDescription]=None\n",
    "\n",
    "\n",
    "############ UTILS ############\n",
    "\n",
    "\n",
    "def assert_dir_exists(dir_name):\n",
    "    dirs=dir_name.split(\"/\")\n",
    "    for i in range(len(dirs)):\n",
    "        prev_dir=\"/\".join(dirs[:i])\n",
    "        cur_dir=\"/\".join(dirs[:i+1])\n",
    "        if not os.path.isdir(cur_dir):\n",
    "            error_str=\"\\n------------------------------------------\\n\"\n",
    "            error_str+=\"Error:\\n\"\n",
    "            error_str+=f\"dir_name does not exist: {dir_name}\\n\\n\"\n",
    "            error_str+=f\"Directory exists: {prev_dir}\\n\\n\"\n",
    "            error_str+=f\"Extension does not exist: {dir_name[len(prev_dir)+1:]}\\n\"\n",
    "            error_str+=f\"To check:\\n\"\n",
    "            error_str+=f\"ls {prev_dir}\"\n",
    "            raise Exception(error_str)\n",
    "        \n",
    "\n",
    "\n",
    "def clean_str_for_os(str_to_clean:str):\n",
    "    str_to_clean=str_to_clean.strip()\n",
    "    chars = [\"=\",\",\",\":\", \"__\", \"-\", \"/\"]\n",
    "    for char in chars:\n",
    "        str_to_clean=str_to_clean.replace(char,\"_\")\n",
    "    return str_to_clean\n",
    "\n",
    "\n",
    "def get_run_folder(root_folder:str, num_beams:int, model:str, task_name: str, suite_name:str):\n",
    "    \n",
    "    num_beams=clean_str_for_os(str(num_beams))\n",
    "    model=clean_str_for_os(model)\n",
    "    task_name=clean_str_for_os(task_name)\n",
    "    suite_name=clean_str_for_os(suite_name)\n",
    "\n",
    "    run_folder= f\"{root_folder}/{suite_name}/{task_name}/{model}/{num_beams}_beams/runs/{suite_name}\"\n",
    "    assert_dir_exists(run_folder)\n",
    "    return run_folder\n",
    "\n",
    "\n",
    "\n",
    "############ Gen Summary stuff ############\n",
    "\n",
    "def clean_generation_summary(generationSummary:GenerationSummary)->GenerationSummary:\n",
    "    def clean_instance_generation(instanceGenerations:InstanceGenerations)->InstanceGenerations:\n",
    "        def clean_generated_output(generatedOutput:GeneratedOutput)-> GeneratedOutput:\n",
    "            generatedOutput.text=truncate_sequence(generatedOutput.text)\n",
    "            generatedOutput=fix_example_themis(generatedOutput)\n",
    "            return generatedOutput\n",
    "        # print(f\"examples len is {len(instanceGenerations.examples)}\")\n",
    "        instanceGenerations.examples=[clean_generated_output(generatedOutput=example) for example in instanceGenerations.examples]\n",
    "        instanceGenerations.examples.sort(key=lambda x:float(x.logprob),reverse=True)\n",
    "        completion=instanceGenerations.examples[0]\n",
    "        instanceGenerations.completion=completion.text\n",
    "        instanceGenerations.completion_logprob=completion.logprob\n",
    "        return instanceGenerations\n",
    "    generationSummary.instance_generations=[clean_instance_generation(instanceGenerations=instance_generation) for instance_generation in generationSummary.instance_generations]\n",
    "    # assert len(generationSummary.instance_generations)==eval_instances\n",
    "    # print(f\"number of instances: {len(generationSummary.instance_generations)}\")\n",
    "    return generationSummary\n",
    "\n",
    "\n",
    "\n",
    "def get_gen_summary_from_path(path) -> GenerationSummary:\n",
    "    # print(f\"path is {path}\")\n",
    "    def json_to_instance_generation(instance_dict:dict) -> InstanceGenerations:\n",
    "        def json_to_generated_output(generated_output_dict):\n",
    "            generated_output=GeneratedOutput(**generated_output_dict)\n",
    "            tokens = [Token(**token) for token in generated_output.tokens]\n",
    "            generated_output.tokens=tokens\n",
    "            return generated_output\n",
    "        instance_generation = InstanceGenerations(**instance_dict)\n",
    "        examples = [ json_to_generated_output(generated_output_dict) for generated_output_dict in instance_generation.examples]\n",
    "        instance_generation.examples=examples\n",
    "        return instance_generation\n",
    "    # print(f\"Getting gen summary from: {path}\")\n",
    "    with open(path,'r') as json_file:\n",
    "        generation_summary_dict=json.load(json_file)\n",
    "    generation_summary=GenerationSummary(**generation_summary_dict)\n",
    "    instance_generations = [json_to_instance_generation(instance_dict)  for instance_dict in generation_summary.instance_generations ]\n",
    "    generation_summary.instance_generations=instance_generations\n",
    "\n",
    "    generation_summary=clean_generation_summary(generation_summary)\n",
    "    return generation_summary\n",
    "\n",
    "\n",
    "def get_gen_summary_from_run_folder(run_folder: str):\n",
    "    gen_sum_raw_path=f\"{run_folder}/generation_summary.json\"\n",
    "    gen_sum_metric_path=f\"{run_folder}/generation_summary_metrics.json\"\n",
    "    input_path = gen_sum_metric_path if os.path.isfile(gen_sum_metric_path) else gen_sum_raw_path\n",
    "    generation_summary=get_gen_summary_from_path(input_path)\n",
    "    return generation_summary\n",
    "\n",
    "\n",
    "def truncate_sequence(text:str, all_stops=[\"<|end_of_text|>\"]) -> str:\n",
    "    for stop in all_stops:\n",
    "        try:\n",
    "            text = text[: text.index(stop)]\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "\n",
    "def append_to_dict(dict, key_list, value):\n",
    "    cur_key=key_list[0]\n",
    "    \n",
    "    #make sure it exists\n",
    "    if cur_key not in dict.keys():\n",
    "        dict[cur_key]={}\n",
    "\n",
    "    #append recursively if not\n",
    "    if(len(key_list)>1):\n",
    "        append_to_dict(dict[cur_key], key_list[1:], value)\n",
    "    else:\n",
    "        dict[cur_key]=value\n",
    "\n",
    "def calculate_dict(init_dict, root_folder, num_beams_list:List[int], models:List[float], task_names:List[str], suite_name:str, dict_function)->Dict[int, GenerationSummary]:\n",
    "    for model in models:        \n",
    "        for task_name in task_names:\n",
    "            for num_beams in num_beams_list:\n",
    "                run_folder=get_run_folder(root_folder=root_folder, num_beams=num_beams, model=model, task_name=task_name, suite_name=suite_name)\n",
    "                obj=dict_function(run_folder)\n",
    "                append_to_dict(init_dict, [suite_name, model, task_name, num_beams], obj)\n",
    "    return init_dict\n",
    "\n",
    "\n",
    "def calculate_instance_stats_dict(init_dict, root_folder, num_beams_list:List[int], models:List[float], task_names:List[str], suite_name:str, instance_metrics:List[str])->Dict[int, List[PerInstanceStats]]:\n",
    "    def json_to_run_instance_stats(run_folder, instance_metrics) -> List[PerInstanceStats]:\n",
    "        path=run_folder+\"/per_instance_stats.json\"\n",
    "        # print(f\"Analyzing path: {path}\")\n",
    "        if not os.path.isfile(path):\n",
    "            return None\n",
    "        with open(path,'r') as json_file:\n",
    "            list_instance_stats_dicts=json.load(json_file)\n",
    "        \n",
    "        instance_id_to_stats_dict={}\n",
    "        for list_instance_stats_dict in list_instance_stats_dicts:\n",
    "            per_instance_stats = PerInstanceStats(**list_instance_stats_dict)\n",
    "            stats = [Stat(**stat_dict) for stat_dict in per_instance_stats.stats]\n",
    "            per_instance_stats.stats=stats\n",
    "            stats_dict={}\n",
    "            for stat in per_instance_stats.stats:\n",
    "                name = stat.name\n",
    "                if name[\"name\"] in instance_metrics and name[\"split\"]==\"test\" and \"perturbation\" not in name.keys():\n",
    "                    stats_dict[name[\"name\"]]= stat.mean\n",
    "            instance_id_to_stats_dict[per_instance_stats.instance_id]=stats_dict\n",
    "        return instance_id_to_stats_dict\n",
    "    dict_function = lambda run_folder: json_to_run_instance_stats(run_folder=run_folder, instance_metrics=instance_metrics)\n",
    "    return calculate_dict(init_dict, root_folder, num_beams_list, models, task_names, suite_name, dict_function)\n",
    "\n",
    "def calculate_instances_dict(init_dict, root_folder, num_beams_list:List[int], models:List[float], task_names:List[str], suite_name:str):\n",
    "    def get_instance_dict_from_run_folder(run_folder):\n",
    "        gen_summary= get_gen_summary_from_run_folder(run_folder)\n",
    "        instance_dict={}\n",
    "        for instance_generation in gen_summary.instance_generations:\n",
    "            instance_dict[instance_generation.instance_id] = instance_generation\n",
    "        return instance_dict\n",
    "    return calculate_dict(init_dict,root_folder, num_beams_list, models, task_names, suite_name,get_instance_dict_from_run_folder )\n",
    "\n",
    "\n",
    "def get_metrics_dict(instances_dict:Dict[int, GenerationSummary], custom_metrics:List[PostMetric.PostMetric], instance_stats_dict):\n",
    "\n",
    "    base_metrics=[PostMetric.TextMetric,PostMetric.SentenceLenMetric(),PostMetric.OutputProbMetric(),\n",
    "                   PostMetric.InstanceIdMetric(), PostMetric.IsCompletionMetric()]\n",
    "    metrics=base_metrics+custom_metrics\n",
    "    metrics_dicts=[]   \n",
    "\n",
    "    for suite_name in instances_dict.keys():\n",
    "        for model in instances_dict[suite_name].keys():        \n",
    "            for task_name in instances_dict[suite_name][model].keys():\n",
    "                for beam_num in instances_dict[suite_name][model][task_name].keys():\n",
    "\n",
    "                    instance_stats_per_run = instance_stats_dict[suite_name][model][task_name][beam_num]\n",
    "\n",
    "                    for instance_id, instance_generation in instances_dict[suite_name][model][task_name][beam_num].items():\n",
    "                        for example_idx,generated_output in enumerate(instance_generation.examples):\n",
    "                            pd_metrics_dict=generated_output.stats_dict if generated_output.stats_dict  is not None else {} \n",
    "                            \n",
    "                            pd_metrics_dict[\"beam_num\"]=beam_num\n",
    "                            pd_metrics_dict[\"task_name\"]=task_name\n",
    "                            pd_metrics_dict[\"model\"]=model\n",
    "                            pd_metrics_dict[\"example_idx\"]=example_idx\n",
    "                            \n",
    "                            #fill out the metrics dict\n",
    "                            for metric in metrics:\n",
    "                                pd_metrics_dict=PostMetric.calculate_post_metric(pd_metrics_dict,metric,instance_generation,generated_output)\n",
    "                            \n",
    "                            if(example_idx==0):\n",
    "                                pd_metrics_dict[\"isCompletion\"]=(example_idx==0)\n",
    "                                if(instance_stats_per_run and instance_generation.instance_id in instance_stats_per_run.keys()):\n",
    "                                    completion_metrics_dict = instance_stats_per_run[instance_generation.instance_id]\n",
    "                                    for stat_name, value in completion_metrics_dict.items():\n",
    "                                        pd_metrics_dict[stat_name]= value\n",
    "                            metrics_dicts.append(pd_metrics_dict)\n",
    "    return metrics_dicts\n",
    "\n",
    "get_first = lambda x: next(iter(x.values()))\n",
    "# @classmethod  \n",
    "# def get_instance_info(self, root_folder, num_beams_list:List[int], models:List[str], task_name: str, suite_name:str)->Dict[int, GenerationSummary]:\n",
    "#     num_beams=num_beams_list[0]\n",
    "#     model=models[0]\n",
    "#     instance_infos= {}\n",
    "#     instance_metrics=[PostMetric.ReferenceMetric()]\n",
    "\n",
    "#     generation_summary=get_gen_summary(root_folder=root_folder, num_beams=num_beams, model=model, task_name=task_name, suite_name=suite_name)\n",
    "#     for instance_generation in generation_summary.instance_generations:\n",
    "#         instance_id=instance_generation.instance_id\n",
    "#         if instance_id not in instance_infos.keys():\n",
    "#             instance_dict={}\n",
    "#             for metric in instance_metrics:\n",
    "#                 instance_dict=PostMetric.calculate_post_metric(metrics_dict=instance_dict,metric=metric,instance_generation=instance_generation,generated_output=None)\n",
    "#             instance_infos[instance_id]=instance_dict\n",
    "#     return instance_infos\n",
    "\n",
    "\n",
    "# @classmethod  \n",
    "# def get_metrics_df(self, root_folder):\n",
    "\n",
    "#     try:\n",
    "#         metrics_file=f\"{root_folder}/metrics_csv.txt\"\n",
    "#         raw_metric_df = pd.read_csv(metrics_file, header=None)\n",
    "#         raw_metric_df.columns=[ \"model\", \"task\", \"beam_num\", \"metric\", \"value\"]\n",
    "#         raw_metric_df.drop([\"task\"],axis=1)\n",
    "#         metric_df = raw_metric_df.pivot(\n",
    "#             index=[\"model\", \"beam_num\"],\n",
    "#             columns=\"metric\",\n",
    "#             values=\"value\"\n",
    "#         ).reset_index()\n",
    "#         metric_df.sort_values(\"beam_num\")\n",
    "#         self.metric_df=metric_df\n",
    "#         return metric_df\n",
    "#     except:\n",
    "#         return None\n",
    "\n",
    "\n",
    "\n",
    "class ProcessGens:\n",
    "    root_folder:str\n",
    "    # task_and_beam_num_to_summary:Dict[int, GenerationSummary]\n",
    "    instances_dict={}\n",
    "    instance_stats_dict={}\n",
    "    metrics_dict:List[Dict[str,any]]=[]\n",
    "    first_run_instances={}\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        self.instances_dict={}\n",
    "        self.instance_stats_dict={}\n",
    "        pass\n",
    "\n",
    "    def init_with_mode(self, process_gens_modes:List[str]):\n",
    "        print(f\"Init: process_gens_mode {process_gens_modes}\")\n",
    "        if isinstance(process_gens_modes, str):\n",
    "            process_gens_modes = [process_gens_modes]\n",
    "        for process_gens_mode in process_gens_modes:\n",
    "            root_folder, num_beams_list, models, custom_metrics, task_names, suite_name, instance_metrics= get_process_gen_params(process_gens_mode)\n",
    "            self.init(root_folder=root_folder,num_beams_list=num_beams_list,models=models,custom_metrics=custom_metrics,task_names=task_names,  suite_name=suite_name,instance_metrics=instance_metrics)\n",
    "            \n",
    "    def get_params(self):\n",
    "        root_folder     =self.process_gen_params[\"root_folder\"]\n",
    "        num_beams_list  =self.process_gen_params[\"num_beams_list\"]\n",
    "        models          =self.process_gen_params[\"models\"]\n",
    "        custom_metrics  =self.process_gen_params[\"custom_metrics\"]\n",
    "        task_names      =self.process_gen_params[\"task_names\"]\n",
    "        suite_name      =self.process_gen_params[\"suite_name\"]\n",
    "        instance_metrics=self.process_gen_params[\"instance_metrics\"]\n",
    "        return root_folder, num_beams_list, models, custom_metrics, task_names, suite_name, instance_metrics\n",
    "\n",
    "            \n",
    "\n",
    "        # firstkey=next(iter(self.instances_dict.keys()))\n",
    "        # print(firstkey)\n",
    "        # secondKey=next(iter(self.instances_dict[firstkey].keys()))\n",
    "        # print(secondKey)\n",
    "        # thirdKey=next(iter(self.instances_dict[firstkey][secondKey].keys()))\n",
    "        # print(thirdKey)\n",
    "        # fourthKey=next(iter(self.instances_dict[firstkey][secondKey][thirdKey].keys()))\n",
    "        # print(fourthKey)\n",
    "        # print(\"First prompt\")\n",
    "        # print(processGens.instances_dict[firstkey][secondKey][thirdKey][fourthKey].prompt)\n",
    "\n",
    "        # print(processGens.instances_dict[0][0][2][\"id10944\"].prompt)\n",
    "\n",
    "\n",
    "    def init(self,root_folder:str, num_beams_list:List[int], models:List[float], custom_metrics:List[PostMetric.PostMetric],task_names:List[str], instance_metrics:Dict[int, Dict[str, Dict[str, float]]]=None, suite_name:str=\"\"):\n",
    "        \n",
    "        # #this is the pre-computed metrics\n",
    "        # print(\"get_metrics_df\")\n",
    "        # self.metrics_df=self.get_metrics_df(root_folder)\n",
    "        # print(\"get_instance_info\")\n",
    "\n",
    "        # #this is th\n",
    "        # instance_info=self.get_instance_info(root_folder=root_folder, num_beams_list=num_beams_list, models=models,task_name= task_name,suite_name=suite_name)\n",
    "        # self.instance_info=instance_info\n",
    "\n",
    "        #get the generation summary for each task beam\n",
    "        print(\"calculate_gen_summary_dict\")\n",
    "        \n",
    "        \n",
    "        self.instances_dict=calculate_instances_dict(init_dict=self.instances_dict, root_folder=root_folder, num_beams_list=num_beams_list, models=models,task_names=task_names, suite_name=suite_name)\n",
    "        \n",
    "\n",
    "        #get the run instance stats for each task and beam\n",
    "        self.instance_stats_dict=calculate_instance_stats_dict(init_dict=self.instance_stats_dict, root_folder=root_folder, num_beams_list=num_beams_list, models=models, task_names=task_names, suite_name=suite_name, instance_metrics=instance_metrics)\n",
    "        \n",
    "\n",
    "        print(\"get_metrics_dict\")\n",
    "        self.metrics_dicts=get_metrics_dict(instances_dict=self.instances_dict, custom_metrics=custom_metrics, instance_stats_dict=self.instance_stats_dict)\n",
    "\n",
    "\n",
    "        self.first_run_instances=get_first(get_first(get_first(self.instances_dict)))\n",
    "        self.ids= list(self.first_run_instances.keys())\n",
    "\n",
    "\n",
    "        self.process_gen_params = {\"root_folder\":root_folder,\n",
    "            \"num_beams_list\":num_beams_list,\n",
    "            \"models\":models,\n",
    "            \"custom_metrics\":custom_metrics,\n",
    "            \"task_names\":task_names,\n",
    "            \"suite_name\":suite_name,\n",
    "            \"instance_metrics\":instance_metrics\n",
    "        }\n",
    "        \n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_metrics_dict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/crfm-helm2/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/miniconda3/envs/crfm-helm2/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/miniconda3/envs/crfm-helm2/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# from process_gens import *\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from helm.benchmark.runner import InstanceGenerations,GenerationSummary\n",
    "from typing import Any, List\n",
    "import json\n",
    "from helm.common.request import (GeneratedOutput, Token)\n",
    "\n",
    "import PostMetric\n",
    "import pandas as pd\n",
    "\n",
    "from helm.benchmark.metrics.statistic import Stat\n",
    "from typing import Dict, Optional\n",
    "\n",
    "from helm.benchmark.augmentations.perturbation_description import (\n",
    "    PerturbationDescription)\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from process_gen_utils import *\n",
    "processGens=ProcessGens()\n",
    "\n",
    "process_gen_modes=\"wmt_single\" # \"wmt_single_top_k_2\"]\n",
    "compare_metric=\"example_comet\"\n",
    "\n",
    "\n",
    "        \n",
    "root_folder, num_beams_list, models, custom_metrics, task_names, suite_name, instance_metrics= get_process_gen_params(process_gen_modes)\n",
    "\n",
    "processGens.instances_dict=calculate_instances_dict(init_dict=processGens.instances_dict, root_folder=root_folder, num_beams_list=num_beams_list, models=models,task_names=task_names, suite_name=suite_name)\n",
    "\n",
    "#get the run instance stats for each task and beam\n",
    "processGens.instance_stats_dict=calculate_instance_stats_dict(init_dict=processGens.instance_stats_dict, root_folder=root_folder, num_beams_list=num_beams_list, models=models, task_names=task_names, suite_name=suite_name, instance_metrics=instance_metrics)\n",
    "\n",
    "\n",
    "print(\"get_metrics_dict\")\n",
    "processGens.metrics_dicts=get_metrics_dict(instances_dict=processGens.instances_dict, custom_metrics=custom_metrics, instance_stats_dict=processGens.instance_stats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: process_gens_mode ['wmt_single', 'wmt_single_top_k_2']\n",
      "calculate_gen_summary_dict\n",
      "get_metrics_dict\n",
      "calculate_gen_summary_dict\n",
      "get_metrics_dict\n",
      "Index(['example_comet', 'beam_num', 'task_name', 'model', 'example_idx',\n",
      "       'text', 'completion_length', 'output_logprob', 'instanceID',\n",
      "       'isCompletion', 'BLEU_1', 'BLEU_4'],\n",
      "      dtype='object')\n",
      "Num examples: 10200\n",
      "Num completions: 1020\n",
      "1020\n"
     ]
    }
   ],
   "source": [
    "\n",
    "process_gen_modes=[\"wmt_single\", \"wmt_single_top_k_2\"]\n",
    "processGens.init_with_mode(process_gen_modes)\n",
    "\n",
    "examples_df, completions_df=get_dfs(processGens)\n",
    "print(len(completions_df))\n",
    "\n",
    "\n",
    "pivoted = result.pivot(index='instanceID', columns='suite', values=compare_metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crfm-helm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
