{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing generate\n",
      "<class 'torch.Tensor'>\n",
      "['Hugging Face is an open-source company that provides a wide range of services to the public. We are a company that is dedicated to providing']\n",
      "tensor([[48098,  2667, 15399,   318,   281,  1280,    12, 10459,  1664,   326,\n",
      "          3769,   257,  3094,  2837,   286,  2594,   284,   262,  1171,    13,\n",
      "           775,   389,   257,  1664,   326,   318,  7256,   284,  4955]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# model_name=\"distilbert/distilgpt2\"\n",
    "# model_name=\"sshleifer/tiny-gpt2\"\n",
    "model_name=\"EleutherAI/gpt-neo-125M\"\n",
    "# model_name= \"google-t5/t5-small\"\n",
    "\n",
    "# model_name=\"zatochu/GPT2-Medium-Alpaca-355m-ggml\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "# t5_model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "encoded_input = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(device)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# eos = tokenizer(tokenizer.eos_token, return_tensors=\"pt\", return_token_type_ids=False).input_ids.flatten()[0].item()\n",
    "# eos = torch.tensor([eos]).to(device).reshape(1,1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"testing generate\")\n",
    "gen_outputs = model.generate(**encoded_input)\n",
    "print(type(gen_outputs))\n",
    "print(tokenizer.batch_decode(gen_outputs))\n",
    "print(gen_outputs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_mode_algo(model, encoded_input, eos:int):\n",
    "    def get_next_log_probs(y, model):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=y)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        return torch.nn.functional.log_softmax(next_token_logits, dim=-1)[0]\n",
    "\n",
    "    #y is the prompt with additions, p is the probability of y, gamma is the current best probability, eos is the eos string\n",
    "    def DFS(  y:str, p:float, gamma:float,model, eos=1, depth=0, max_depth=-1):\n",
    "\n",
    "\n",
    "        \n",
    "        #If we reached max depth without finishing\n",
    "        if(depth>max_depth):\n",
    "            return (y,gamma*2)\n",
    "\n",
    "        #if y is finished, return the node\n",
    "        if(y[0,-1]==eos):\n",
    "            print(f\"p is {p}, y is {tokenizer.batch_decode(y)}, gamma is {gamma}\", flush=True)\n",
    "            return (y,p)\n",
    "        \n",
    "        #else, search for the best way to complete y\n",
    "        \n",
    "        #exclude the pad token\n",
    "        log_probs=get_next_log_probs(y, model)\n",
    "\n",
    "        arange=torch.arange(len(log_probs)).to(log_probs.device)\n",
    "        \n",
    "        best_y=None\n",
    "        for idx, log_prob in enumerate(log_probs):\n",
    "            newP = p + log_prob \n",
    "            #if we're doing better than the best one so far\n",
    "            if newP > gamma:\n",
    "\n",
    "                # print(f\"gamma is {gamma}, newP is {newP}\")\n",
    "                #do a DFS\n",
    "                appended_y=torch.concat((y, arange[idx].reshape(1,1)), axis=1)\n",
    "                new_y, new_gamma = DFS( y=appended_y, p=newP, gamma=gamma, model=model, eos=eos, depth=depth+1, max_depth=max_depth)\n",
    "                if new_gamma > gamma:\n",
    "                    best_y=new_y\n",
    "                    gamma=new_gamma\n",
    "\n",
    "        return best_y, gamma\n",
    "\n",
    "    y=encoded_input.input_ids\n",
    "    ended_y=torch.concat((y, eos), axis=1)\n",
    "    start_gamma=get_next_log_probs(y=y, model=model)[eos]\n",
    "\n",
    "\n",
    "\n",
    "    best_y, gamma = DFS(y=y, gamma=start_gamma,p=0, model= model, eos=eos, depth=0, max_depth=100)\n",
    "    if best_y is None: \n",
    "        return ended_y, gamma\n",
    "    return best_y, gamma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: -1.0048828125,index: 326 token:  that\n",
      "p: -2.41796875,index: 11 token: ,\n",
      "p: -3.244140625,index: 351 token:  with\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'that'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_next_log_probs(y, model):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=y)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    next_token_logits = logits[:, -1, :]\n",
    "    return torch.nn.functional.log_softmax(next_token_logits, dim=-1)[0]\n",
    "\n",
    "next_log_probs=get_next_log_probs(encoded_input.input_ids, model)\n",
    "\n",
    "a=torch.topk(next_log_probs, 3)\n",
    "\n",
    "for index, value in zip(a.indices, a.values):\n",
    "    print(f\"p: {value},index: {index} token: {tokenizer.decode([index])}\")\n",
    "\n",
    "tokenizer.decode(tokenizer.encode(\"that\"))\n",
    "\n",
    "#so, start_Gamma should be -1.0048828125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eos is tensor([[13]])\n",
      "p is -8.734375, y is ['Hugging Face is an open-source company that creates open-source software.'], gamma is tensor([[-9.1406]], dtype=torch.float16)\n",
      "p is -8.5546875, y is ['Hugging Face is an open-source company that creates open source software.'], gamma is -8.734375\n",
      "gamma is -8.5546875, best_y is ['Hugging Face is an open-source company that creates open source software.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoded_input = tokenizer(\"Hugging Face is an open-source company that creates\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "eos = torch.tensor([tokenizer.encode(\".\")]).to(device).reshape(1,1)\n",
    "print(f\"eos is {eos}\")\n",
    "# print(f\"eos is {eos}\")\n",
    "\n",
    "\n",
    "# start_gamma=get_next_log_probs(y=encoded_input.input_ids, model=model)[eos]\n",
    "# print(f\"1. start_gamma is {start_gamma}\")\n",
    "\n",
    "best_y, gamma = exact_mode_algo(model, encoded_input, eos)\n",
    "print(f\"gamma is {gamma.item()}, best_y is {tokenizer.batch_decode(best_y)}\")\n",
    "\n",
    "# tokenizer.decode(best_y)\n",
    "\n",
    "\n",
    "\n",
    "# get_decode_log_prob(x,gen_outputs, t5_model, tokenizer)\n",
    "\n",
    "\n",
    "# print(\"testing blank string\")\n",
    "# get_decode_log_prob(x,ended_y, t5_model, tokenizer)\n",
    "\n",
    "\n",
    "# 48098,  2667, 15399,   318,   281,  1280,    12, 10459,  1664,   \n",
    "    # 326, 3769,   257,  3094,  2837,   286,  2594,   284,   262,  1171,    13, 775,   389,   257,  1664,   326,   318,  7256,   284,  4955"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hugging Face is an open-source company that creates open source software.']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(best_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crfm-helm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
